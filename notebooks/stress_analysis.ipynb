{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/ai-env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at xmj2002/hubert-base-ch-speech-emotion-recognition were not used when initializing HubertForSpeechClassification: ['hubert.encoder.pos_conv_embed.conv.weight_g', 'hubert.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertForSpeechClassification were not initialized from the model checkpoint at xmj2002/hubert-base-ch-speech-emotion-recognition and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "from transformers import AutoConfig, Wav2Vec2FeatureExtractor, HubertPreTrainedModel, HubertModel\n",
    "\n",
    "\n",
    "model_name  = \"xmj2002/hubert-base-ch-speech-emotion-recognition\"\n",
    "duration    = 15\n",
    "sample_rate = 16000\n",
    "model_id    = 150\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "class HubertForSpeechClassification(HubertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Hubert model for speech classification.\n",
    "\n",
    "    Args:\n",
    "        config (HubertConfig): The model configuration class instance.\n",
    "\n",
    "    Attributes:\n",
    "        hubert (HubertModel): The Hubert model.\n",
    "        classifier (HubertClassificationHead): The classification head.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.hubert = HubertModel(config)\n",
    "        self.classifier = HubertClassificationHead(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the HubertForSpeechClassification model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor.\n",
    "        \"\"\"\n",
    "        outputs = self.hubert(x)\n",
    "        hidden_states = outputs[0]\n",
    "        x = torch.mean(hidden_states, dim=1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class HubertClassificationHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes the HubertClassificationHead module.\n",
    "\n",
    "        Args:\n",
    "            config (object): Configuration object containing the model's hyperparameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.classifier_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs forward pass of the HubertClassificationHead module.\n",
    "\n",
    "        Args:\n",
    "            x (tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model = HubertForSpeechClassification.from_pretrained(\n",
    "            model_name,\n",
    "            config=config,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2class(id):\n",
    "    \"\"\"\n",
    "    Convert class ID to corresponding emotion class.\n",
    "\n",
    "    Args:\n",
    "        id (int): Class ID.\n",
    "\n",
    "    Returns:\n",
    "        str: Emotion class.\n",
    "    \"\"\"\n",
    "    if id == 0:\n",
    "        return \"angry\"\n",
    "    elif id == 1:\n",
    "        return \"fear\"\n",
    "    elif id == 2:\n",
    "        return \"happy\"\n",
    "    elif id == 3:\n",
    "        return \"neutral\"\n",
    "    elif id == 4:\n",
    "        return \"sadness\"\n",
    "    else:\n",
    "        return \"excited\"\n",
    "\n",
    "\n",
    "def predict(audio_path):\n",
    "    \"\"\"\n",
    "    Predict the emotion class of an audio file.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file.\n",
    "        processor: Feature extractor for audio processing.\n",
    "        model: Speech classification model.\n",
    "\n",
    "    Returns:\n",
    "        str: Emotion class prediction.\n",
    "    \"\"\"\n",
    "    speech, sr  = librosa.load(path=audio_path, sr=sample_rate)\n",
    "    speech      = processor(speech, padding=\"max_length\", truncation=True, max_length=duration * sr, return_tensors=\"pt\", sampling_rate=sr).input_values\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logit = model(speech)\n",
    "\n",
    "    score   = F.softmax(logit, dim=1).detach().cpu().numpy()[0]\n",
    "    id      = torch.argmax(logit).cpu().numpy()\n",
    "\n",
    "    return id2class(id), score[id]\n",
    "\n",
    "\n",
    "def predict_emotion(audio_buffer):\n",
    "    \"\"\"\n",
    "    Predict the emotion class of an audio file.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "        str: Emotion class prediction.\n",
    "    \"\"\"\n",
    "    stress_result, confidence_value = predict(audio_buffer)\n",
    "    return stress_result, confidence_value\n",
    "\n",
    "\n",
    "def get_audio_duration(data):\n",
    "    \"\"\"\n",
    "    Get the duration of an audio file.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "        float: Duration of the audio file in minutes.\n",
    "    \"\"\"\n",
    "    duration = round((len(data) / sample_rate)/60, 2)\n",
    "    return duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('angry', 0.80610543)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_path = '../data/2024-04-22-tait/14senjataapi-1_95Odpk99 1.mp3'\n",
    "emotion_result, confidence_value  = predict_emotion(audio_path)\n",
    "emotion_result, confidence_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_duration  = get_audio_duration(audio_path)\n",
    "audio_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StressAnalysisGenerator:\n",
    "    \"\"\"\n",
    "    Class for generating stress analysis results from audio files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.processor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "        self.model = HubertForSpeechClassification.from_pretrained(\n",
    "            model_name,\n",
    "            config=config,\n",
    "        )\n",
    "    \n",
    "\n",
    "    def id2class(self, id):\n",
    "        \"\"\"\n",
    "        Convert class ID to corresponding emotion class.\n",
    "\n",
    "        Args:\n",
    "            id (int): Class ID.\n",
    "\n",
    "        Returns:\n",
    "            str: Emotion class.\n",
    "        \"\"\"\n",
    "        if id == 0:\n",
    "            return \"angry\"\n",
    "        elif id == 1:\n",
    "            return \"fear\"\n",
    "        elif id == 2:\n",
    "            return \"happy\"\n",
    "        elif id == 3:\n",
    "            return \"neutral\"\n",
    "        elif id == 4:\n",
    "            return \"sadness\"\n",
    "        else:\n",
    "            return \"excited\"\n",
    "\n",
    "\n",
    "    def predict(self, audio_path):\n",
    "        \"\"\"\n",
    "        Predict the emotion class of an audio file.\n",
    "\n",
    "        Args:\n",
    "            audio_path (str): Path to the audio file.\n",
    "            processor: Feature extractor for audio processing.\n",
    "            model: Speech classification model.\n",
    "\n",
    "        Returns:\n",
    "            str: Emotion class prediction.\n",
    "        \"\"\"\n",
    "        speech, sr  = librosa.load(path=audio_path, sr=sample_rate)\n",
    "        speech      = self.processor(speech, padding=\"max_length\", truncation=True, max_length=duration * sr, return_tensors=\"pt\", sampling_rate=sr).input_values\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logit = self.model(speech)\n",
    "\n",
    "        score   = F.softmax(logit, dim=1).detach().cpu().numpy()[0]\n",
    "        id      = torch.argmax(logit).cpu().numpy()\n",
    "\n",
    "        return self.id2class(id), score[id]\n",
    "    \n",
    "    \n",
    "    def predict_emotion(self, audio_buffer):\n",
    "        \"\"\"\n",
    "        Predict the emotion class of an audio file.\n",
    "\n",
    "        Args:\n",
    "            audio_path (str): Path to the audio file.\n",
    "\n",
    "        Returns:\n",
    "            str: Emotion class prediction.\n",
    "        \"\"\"\n",
    "        stress_result, confidence_value = self.predict(audio_buffer)\n",
    "        return stress_result, confidence_value\n",
    "\n",
    "\n",
    "    def get_audio_duration(self, audio_path):\n",
    "        \"\"\"\n",
    "        Get the duration of an audio file.\n",
    "\n",
    "        Args:\n",
    "            audio_path (str): Path to the audio file.\n",
    "\n",
    "        Returns:\n",
    "            float: Duration of the audio file in minutes.\n",
    "        \"\"\"\n",
    "        data, sample_rate = sf.read(audio_path)\n",
    "        duration = round((len(data) / sample_rate)/60, 2)\n",
    "        return duration\n",
    "    \n",
    "    def transcribe(self, audio_buffer):\n",
    "        \"\"\"\n",
    "        Transcribe an audio file and store the results in a database.\n",
    "\n",
    "        Args:\n",
    "            audio_path (str): Path to the audio file.\n",
    "            correlation_id (str): Correlation ID for tracking purposes.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the audio file format is not supported.\n",
    "\n",
    "        \"\"\"\n",
    "        startimestamp   = int(time.time())\n",
    "        start_time      = str(datetime.now(tz=timezone.utc))[:10] + 'T' + str(datetime.now(tz=timezone.utc))[11:19]\n",
    "        emotion_result, confidence_value  = self.predict_emotion(audio_buffer)\n",
    "        audio_duration  = self.get_audio_duration(audio_buffer)\n",
    "        finish_time     = str(datetime.now(tz=timezone.utc))[:10] + 'T' + str(datetime.now(tz=timezone.utc))[11:19]\n",
    "        duration        = round((int(time.time()) - startimestamp) / 60, 2)\n",
    "        df              = pd.DataFrame([[emotion_result, round(confidence_value, 2), audio_duration, start_time, finish_time, duration, datetime.now(tz=timezone.utc)]], \n",
    "                        columns=['emotion_result', 'confidence_value', 'audio_duration', 'start_time', 'finish_time', 'sa_duration', 'inserted_at'])\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xmj2002/hubert-base-ch-speech-emotion-recognition were not used when initializing HubertForSpeechClassification: ['hubert.encoder.pos_conv_embed.conv.weight_g', 'hubert.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertForSpeechClassification were not initialized from the model checkpoint at xmj2002/hubert-base-ch-speech-emotion-recognition and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion_result</th>\n",
       "      <th>confidence_value</th>\n",
       "      <th>audio_duration</th>\n",
       "      <th>start_time</th>\n",
       "      <th>finish_time</th>\n",
       "      <th>sa_duration</th>\n",
       "      <th>inserted_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.76</td>\n",
       "      <td>2024-06-12T01:24:47</td>\n",
       "      <td>2024-06-12T01:24:48</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2024-06-12 01:24:48.338073+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emotion_result  confidence_value  audio_duration           start_time  \\\n",
       "0          angry              0.81            0.76  2024-06-12T01:24:47   \n",
       "\n",
       "           finish_time  sa_duration                      inserted_at  \n",
       "0  2024-06-12T01:24:48         0.02 2024-06-12 01:24:48.338073+00:00  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = StressAnalysisGenerator()\n",
    "df = a.transcribe(audio_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
